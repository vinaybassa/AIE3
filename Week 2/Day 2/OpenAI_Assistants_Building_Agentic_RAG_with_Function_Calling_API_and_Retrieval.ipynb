{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgDepNVhvzIr"
      },
      "source": [
        "# OpenAI Assistants - Building Agentic RAG with the Function Calling, Retrieval, and Code Interpreter Tools\n",
        "\n",
        "Today we'll explore using OpenAI's Python SDK to create, manage, and use the OpenAI Assistant API!\n",
        "\n",
        "We'll be doing the following in today's notebook:\n",
        "\n",
        "1. Task 1: Simple Assistant\n",
        "2. Task 2: Adding Tools\n",
        "  - Task 2a: Creating an Assistant with File Search Tool\n",
        "  - Task 2b: Creating an Assistant with Code Interpreter Tool\n",
        "  - Task 2c: Creating an Assistant with Function Calling Tool\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Colab Specific Instructions:\n",
        "\n",
        "To get started, please make a copy of this notebook using `File > Save a copy in Drive`\n",
        "\n",
        "![image](https://i.imgur.com/rNzMEfs.png)\n",
        "\n",
        "You will be expected to submit a GitHub link to the completed notebook, so if you're completing the assignment on Colab - you'll want to download the notebook when you have completed it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNU6b3ymwOWq"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "We'll start, as we usually do, with some dependiencies and our API key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePayyL6at6LS",
        "outputId": "4fb742b3-c650-44c2-8c58-2d83c81a766b"
      },
      "outputs": [],
      "source": [
        "!pip install -qU openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unKr3HZdu-1V",
        "outputId": "9c567820-9633-44fd-dad6-e7dd1ca46d97"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwNE7N4HwhXH"
      },
      "source": [
        "## Task 1: Simple Assistant\n",
        "\n",
        "Let's create a simple Assistant to understand more about how the API works to start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKEwbLMFxNKs"
      },
      "source": [
        "### OpenAI Client\n",
        "\n",
        "At the core of the OpenAI Python SDK is the Client!\n",
        "\n",
        "> NOTE: For ease of use, we'll start with the synchronous `OpenAI()`. OpenAI does provide an `AsyncOpenAI()` that you could leverage as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wF-mBZwtuavl"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIx4GZ2w_1c"
      },
      "source": [
        "### Creating An Assistant\n",
        "\n",
        "Leveraging what we know about the OpenAI API from previous sessions - we're going to start by simply initializing an Assistant.\n",
        "\n",
        "Before we begin, we need to think about a few customization options we have:\n",
        "\n",
        "- `name` - Straight forward enough, this is what our Assistant's name will be\n",
        "- `instructions` - similar to a system message, but applied at an Assistant level, this is how we can guide the Assistant's tone, behaviour, functionality, and more!\n",
        "- `model` - this will allow us to choose which model we would prefer to use for our Assistant\n",
        "\n",
        "Let's start by setting some instructions for our Assistant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "Paqd6zWMyMAJ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### 🏗️ Build Activity 🏗️\n",
        "# @markdown Fill out the fields below to add your Assistant's name, instructions, and desired model!\n",
        "\n",
        "name = \"Virtual Assistant\" # @param {type: \"string\"}\n",
        "instructions = \"You are a professor in computer science and simple words\" # @param {type: \"string\"}\n",
        "model = \"gpt-4o\" # @param [\"gpt-3.5-turbo\", \"gpt-4-turbo-preview\", \"gpt-4\", \"gpt-4o\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUeaDsLMzcv-"
      },
      "source": [
        "### Initialize Assistant\n",
        "\n",
        "Now that we have our desired name, instruction, and model - we can initialize our Assistant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6-4MgVLbu8rO"
      },
      "outputs": [
        {
          "ename": "APIConnectionError",
          "evalue": "Connection error.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:69\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(request)\n\u001b[1;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:154\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_tls\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 154\u001b[0m     stream \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mstart_tls(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    155\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_backends/sync.py:152\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[0;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[1;32m    148\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    149\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    151\u001b[0m }\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mConnectError\u001b[0m: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1006)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    953\u001b[0m         request,\n\u001b[1;32m    954\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m    955\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:232\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:86\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
            "\u001b[0;31mConnectError\u001b[0m: [SSL: UNSAFE_LEGACY_RENEGOTIATION_DISABLED] unsafe legacy renegotiation disabled (_ssl.c:1006)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAPIConnectionError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m assistant \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39massistants\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      2\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m      3\u001b[0m     instructions\u001b[38;5;241m=\u001b[39minstructions,\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/resources/beta/assistants.py:156\u001b[0m, in \u001b[0;36mAssistants.create\u001b[0;34m(self, model, description, instructions, metadata, name, response_format, temperature, tool_resources, tools, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03mCreate an assistant with a model and instructions.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI-Beta\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistants=v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/assistants\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    158\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    159\u001b[0m         {\n\u001b[1;32m    160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: description,\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: instructions,\n\u001b[1;32m    163\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    164\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: name,\n\u001b[1;32m    165\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    167\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_resources\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_resources,\n\u001b[1;32m    168\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    170\u001b[0m         },\n\u001b[1;32m    171\u001b[0m         assistant_create_params\u001b[38;5;241m.\u001b[39mAssistantCreateParams,\n\u001b[1;32m    172\u001b[0m     ),\n\u001b[1;32m    173\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    174\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    175\u001b[0m     ),\n\u001b[1;32m    176\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mAssistant,\n\u001b[1;32m    177\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    922\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    923\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    924\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    925\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    926\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    927\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:976\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    973\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    977\u001b[0m         options,\n\u001b[1;32m    978\u001b[0m         cast_to,\n\u001b[1;32m    979\u001b[0m         retries,\n\u001b[1;32m    980\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    981\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    982\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    983\u001b[0m     )\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:976\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    973\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    977\u001b[0m         options,\n\u001b[1;32m    978\u001b[0m         cast_to,\n\u001b[1;32m    979\u001b[0m         retries,\n\u001b[1;32m    980\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    981\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    982\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    983\u001b[0m     )\n\u001b[1;32m    985\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1054\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1055\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1056\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1057\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1058\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1059\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:986\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    977\u001b[0m             options,\n\u001b[1;32m    978\u001b[0m             cast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    982\u001b[0m             response_headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    983\u001b[0m         )\n\u001b[1;32m    985\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    988\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    990\u001b[0m     request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m     response\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    995\u001b[0m )\n\u001b[1;32m    996\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx-request-id\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
            "\u001b[0;31mAPIConnectionError\u001b[0m: Connection error."
          ]
        }
      ],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=name,\n",
        "    instructions=instructions,\n",
        "    model=model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QskO5n5W2X6t"
      },
      "source": [
        "Let's examine our `assistant` object and see what we find!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkkIC_JP2bG0",
        "outputId": "86ecb682-a04d-4687-94d3-18a4b99ffc07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Assistant(id='asst_WUooFQcgjaLg1e7nh1Xq9ONZ', created_at=1717712922, description=None, instructions=\"You're a cool guy, who doesn't afraid of anything.\", metadata={}, model='gpt-4o', name='COOL_ASSISTANT', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "615NK1Qj2e_Z"
      },
      "source": [
        "There are a number of useful parameters here, but we'll call out a few:\n",
        "\n",
        "- `id` - since we may have multiple Assistant's, knowing which Assistant we're interacting with will help us ensure the desired user experience!\n",
        "- `description` - A natrual language description of our Assistant could help others understand what it's supposed to do!\n",
        "- `file_ids` - if we wanted to use the Retrieval tool, this would let us know what files we had given our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3HhlqtM0AhW"
      },
      "source": [
        "### Creating a Thread\n",
        "\n",
        "Behind the scenes our Assistant is powered by the idea of \"threads\".\n",
        "\n",
        "You can think of threads as individual conversations that interact with the Assistant.\n",
        "\n",
        "Let's create a thread now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iFVM39vevT5f"
      },
      "outputs": [],
      "source": [
        "thread = client.beta.threads.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y7jelq01PoG"
      },
      "source": [
        "Let's look at our `thread` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V8WAKDZ1Uf2",
        "outputId": "6ba2a680-a595-40e8-fa5d-5784b9614b97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Thread(id='thread_XJDW1thSrFOKqukSc9w30lJ5', created_at=1717712924, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k6S_e501V4z"
      },
      "source": [
        "Notice some key attributes:\n",
        "\n",
        "- `id` - since each Thread is like a conversation, we need some way to specify which thread we're dealing with when interacting with them\n",
        "- `tool_resources` - this will become more relevant as we add tools since we'll need a way to verify which tools we have access to when interacting with our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5BvGv1N0c2h"
      },
      "source": [
        "### Adding Messages to Our Thread\n",
        "\n",
        "Now that we have our Thread (or conversation) we can start adding messages to it!\n",
        "\n",
        "Let's add a simple message that asks about how our Assistant is feeling.\n",
        "\n",
        "Notice the parameters we're leveraging:\n",
        "\n",
        "- `thread_id` - since each Thread is like a conversation, we need some way to address a specific conversation. We can use `thread.id` to do this.\n",
        "- `role` - similar to when we used our chat completions endpoint, this parameter specifies who the message is coming from. You can leverage this in the same ways you would through the chat completions endpoint.\n",
        "- `content` - this is where we can place the actual text our Assistant will interact with\n",
        "\n",
        "> NOTE: Feel free to substitute a relevant message based on the Assistant you created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R7ZNCfGivagg"
      },
      "outputs": [],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=f\"How old are you?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc7R3Sr32P0b"
      },
      "source": [
        "Again, let's examine our `message` object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMLvyZDA2S_D",
        "outputId": "dfde63a0-3b6f-4f01-b76c-ec84baefe5a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Message(id='msg_56ZiiFLj0NwF5GgTYoBwRJfx', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='How old are you?'), type='text')], created_at=1717712979, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_XJDW1thSrFOKqukSc9w30lJ5')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI3Pctpk29og"
      },
      "source": [
        "### Running Our Thread\n",
        "\n",
        "Now that we have an Assistant, and we've given that Assistant a Thread, and we've added a Message to that Thread - we're ready to run our Assistant!\n",
        "\n",
        "Notice that this process lets us add (potentially) multiple messages to our Assistant. We can leverage that behaviour for few/many-shot examples, and more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "VkvsXv5_3cyQ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### 🏗️ Build Activity 🏗️\n",
        "# @markdown We can also override the Assistant's instructions when we run a thread.\n",
        "\n",
        "# @markdown Use one of the [Prompt Principles for Instruction](https://arxiv.org/pdf/2312.16171v1.pdf) to improve the likeliehood of a correct or valuable response from your Assistant.\n",
        "\n",
        "additional_instructions = \"YOUR EXTRA INSTRUCTIONS HERE\" # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CuGbTrL5QEc"
      },
      "source": [
        "Let's run our Thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fpWNl3UVvdW4"
      },
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        "  instructions=additional_instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erMGdU7y6la2"
      },
      "source": [
        "Now that we've run our thread, let's look at the object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz_rfwi869YI",
        "outputId": "56aa670d-8f22-4ef4-b5f4-e568c6f7cefb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Run(id='run_WIP4hFDplMO78DZy6sqx0geA', assistant_id='asst_WUooFQcgjaLg1e7nh1Xq9ONZ', cancelled_at=None, completed_at=None, created_at=1717712992, expires_at=1717713592, failed_at=None, incomplete_details=None, instructions='Be cool man.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o', object='thread.run', parallel_tool_calls=True, required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_XJDW1thSrFOKqukSc9w30lJ5', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h2SH_347JJb"
      },
      "source": [
        "Notice we have access to a few very powerful parameters in this `run` object.\n",
        "\n",
        "- `completed_at` - this will help us determine when we can expect to retrieve a response\n",
        "- `failed_at` - this can highlight any issues our run ran into\n",
        "- `status` - is another way we can understand how the flow is going"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVBNagBU7kpx"
      },
      "source": [
        "### Retrieving Our Run\n",
        "\n",
        "Now that we've created our run, let's retrieve it.\n",
        "\n",
        "We're going to wrap this in a simple loop to make sure we're not retrieving it too early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "itz5_otPvfkV"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "while run.status == \"in_progress\" or run.status == \"queued\":\n",
        "  time.sleep(1)\n",
        "  run = client.beta.threads.runs.retrieve(\n",
        "    thread_id=thread.id,\n",
        "    run_id=run.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgGE1uUJ7z3h",
        "outputId": "89f3df27-d5a3-4095-9422-6271db52cfba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "completed\n"
          ]
        }
      ],
      "source": [
        "print(run.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHVTS4hD7-fv"
      },
      "source": [
        "Now that our run is completed - we can retieve the messages from our thread!\n",
        "\n",
        "Notice that our run helps us understand how things are going - but it isn't where we're going to find our responses or messages. Those are added on the backend into our thread.\n",
        "\n",
        "This leads to a simple, but important, flow:\n",
        "\n",
        "1. We add messages to a thread.\n",
        "2. We create a run on that thread.\n",
        "3. We wait until the run is finished.\n",
        "4. We check our thread for the new messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGBNpGmh-ZpW"
      },
      "source": [
        "### Checking Our Thread\n",
        "\n",
        "Now we can get a list of messages from our thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Av-OQDUPvhAd"
      },
      "outputs": [],
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM6cZk-GviqX",
        "outputId": "3916ed59-7eba-471f-9eb7-d8d2f4658012"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Message(id='msg_ExYEh7aE9amXFUMa6HMGW54D', assistant_id='asst_WUooFQcgjaLg1e7nh1Xq9ONZ', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"I'm not exactly sure how to measure my age the way humans do, but I've been around since my latest update. So, let's just say I'm as old as the newest information I was trained on! Pretty slick, huh?\"), type='text')], created_at=1717712993, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_WIP4hFDplMO78DZy6sqx0geA', status=None, thread_id='thread_XJDW1thSrFOKqukSc9w30lJ5')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages.data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT9_V_XlaVH7"
      },
      "source": [
        "### Streaming Our Runs\n",
        "\n",
        "With recent upgrades to the Assistant API - we can now *stream* our outputs!\n",
        "\n",
        "In order to do this - we'll need something called an `EventHandler` which will help us to decide on what actions to take based on the output of the LLM.\n",
        "\n",
        "Let's build it below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YvH-FUbQawcN"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import override\n",
        "from openai import AssistantEventHandler\n",
        "\n",
        "class EventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_text_delta(self, delta, snapshot):\n",
        "    print(delta.value, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvBieRYvbAmt"
      },
      "source": [
        "Now we can create our `run` and stream the output as it comes in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8ax_0ZfbF_I",
        "outputId": "1bbddea1-a145-4a0d-b662-73f7703a511f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > I'm not exactly sure how to measure my age the way humans do, but I've been around since my latest update. So, let's just say I'm as old as the newest information I was trained on! Pretty slick, huh?"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        "  instructions=additional_instructions,\n",
        "  event_handler=EventHandler(),\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgnY16tjCmc6"
      },
      "source": [
        "## Task 2: Adding Tools\n",
        "\n",
        "Now that we have an understanding of how Assistant works, we can start thinking about adding tools.\n",
        "\n",
        "We'll go through 3 separate tools and explore how we can leverage them!\n",
        "\n",
        "Let's start with the most familiar tool - the Retriever!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0NagnlZC8g9"
      },
      "source": [
        "### Task 2a: Creating an Assistant with the File Search Tool\n",
        "\n",
        "The first thing we'll want to do is create an assistant with the File Search tool.\n",
        "\n",
        "This is also going to require some data. We'll provided data - but you're very much encouraged to use your own files to explore how the Assistant works for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HInYwNiQEjQH"
      },
      "source": [
        "#### Collect and Add Data to Vector Store\n",
        "\n",
        "1. First, we need some data.\n",
        "2. Second, we need to add the data to our Assistant!\n",
        "\n",
        "Let's start with grabbing some data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvAHBszIEa1Y",
        "outputId": "c9e86f6b-89b5-426b-ecd8-a6d23cbea92b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-06 22:30:25--  https://github.com/dbredvick/paul-graham-to-kindle/blob/main/paul_graham_essays.txt\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘paul_graham_essays.txt.2’\n",
            "\n",
            "paul_graham_essays.     [ <=>                ] 153.36K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-06-06 22:30:25 (15.6 MB/s) - ‘paul_graham_essays.txt.2’ saved [157043]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/dbredvick/paul-graham-to-kindle/blob/main/paul_graham_essays.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2EpY1w_FQ3m"
      },
      "source": [
        "Now we can upload our file to our Vector Store!\n",
        "\n",
        "Pay attention to [this](https://platform.openai.com/docs/assistants/tools/file-search/supported-files) documentation to see what kinds of files can be uploaded.\n",
        "\n",
        "> NOTE: Per the OpenAI [docs](https://platform.openai.com/docs/assistants/tools/file-search/vector-stores) The maximum file size is 512 MB and no more than 2,000,000 tokens (computed automatically when you attach a file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6UgUYNTaOpUK"
      },
      "outputs": [],
      "source": [
        "vector_store = client.beta.vector_stores.create(name=\"Paul Graham Essay Compilation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dpVoe2SMFI6s"
      },
      "outputs": [],
      "source": [
        "file_paths = [\"paul_graham_essays.txt\"]\n",
        "file_streams = [open(path, \"rb\") for path in file_paths]\n",
        "\n",
        "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
        "  vector_store_id=vector_store.id, files=file_streams\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBQOSGyyF2u5"
      },
      "source": [
        "Let's look at what our `file_batch` contains!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zJrVLkMpFgwf"
      },
      "outputs": [],
      "source": [
        "while file_batch.status != \"completed\":\n",
        "  time(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd4O4dpZF-eH"
      },
      "source": [
        "#### Create and Use Assistant\n",
        "\n",
        "Now that we have our file - we can attach it to an Assistant, and we can give that Assistant the ability to use it for retrieval through the Retrieval tool!\n",
        "\n",
        "> NOTE: Your first GB is free and beyond that, usage is billed at $0.10/GB/day of vector storage. There are no other costs associated with vector store operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jfn_MlJqFiEe"
      },
      "outputs": [],
      "source": [
        "fs_assistant = client.beta.assistants.create(\n",
        "  name=name,\n",
        "  instructions=instructions,\n",
        "  model=model,\n",
        "  tools=[{\"type\": \"file_search\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "AhXXOXp1eybd"
      },
      "outputs": [],
      "source": [
        "fs_assistant = client.beta.assistants.update(\n",
        "  assistant_id=fs_assistant.id,\n",
        "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JM9iJhoMcUfW"
      },
      "outputs": [],
      "source": [
        "fs_thread = client.beta.threads.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What did Paul Graham say about Silicon Valley?\",\n",
        "    }\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0-mmRjQGeUR"
      },
      "source": [
        "We can use an extension of the `EventHandler` that we created above to stream our `run`!\n",
        "\n",
        "Let's add a few things:\n",
        "\n",
        "1. A `on_tool_call_created` function which tells us which tool is being used.\n",
        "2. A `on_message_done` that includes citations that were used by our File Search tool - this is like return the context *and* the response that we saw in our Pythonic RAG implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "av5A_tm0bpvf"
      },
      "outputs": [],
      "source": [
        "class FSEventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_tool_call_created(self, tool_call):\n",
        "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_message_done(self, message) -> None:\n",
        "    message_content = message.content[0].text\n",
        "    annotations = message_content.annotations\n",
        "    citations = []\n",
        "    for index, annotation in enumerate(annotations):\n",
        "      message_content.value = message_content.value.replace(\n",
        "        annotation.text, f\"[{index}]\"\n",
        "      )\n",
        "      if file_citation := getattr(annotation, \"file_citation\", None):\n",
        "        cited_file = client.files.retrieve(file_citation.file_id)\n",
        "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
        "\n",
        "    print(message_content.value)\n",
        "    print(\"\\n\".join(citations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxJfa-saHbNQ"
      },
      "source": [
        "Let's look at the final result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19nDqjRgHaNA",
        "outputId": "b33c4c2d-e8a7-432c-b596-a2169c487f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > file_search\n",
            "\n",
            "\n",
            "assistant > Paul Graham's writings about Silicon Valley, captured in his essays, convey several insights and observations about the region. He emphasizes the crucial role of risk-taking, innovation, and a supportive ecosystem in fostering startups. Graham asserts that Silicon Valley's unique environment, characterized by a high concentration of ambitious individuals, funding opportunities, and tolerance for failure, creates a fertile ground for new ventures.\n",
            "\n",
            "Here are a few key points mentioned by Paul Graham regarding Silicon Valley:\n",
            "\n",
            "1. **Attraction of Talent**: Silicon Valley draws the best talent from all over the world due to its vibrant startup culture and the potential for substantial rewards.\n",
            "2. **Tolerance for Failure**: The region's acceptance of failure as a stepping stone to success encourages entrepreneurs to take bold risks without the fear of being stigmatized.\n",
            "3. **Access to Capital**: Abundant venture capital funding in Silicon Valley plays a pivotal role in turning innovative ideas into successful startups.\n",
            "\n",
            "These elements combine to make Silicon Valley a hub of technological advancement and entrepreneurial activity.\n",
            "\n",
            "For more detailed insights, you can refer directly to the essays authored by Paul Graham .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=fs_thread.id,\n",
        "  assistant_id=fs_assistant.id,\n",
        "  event_handler=FSEventHandler(),\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pln9uYoJICno"
      },
      "source": [
        "### Task 2b: Creating an Assistant with the Code Interpreter Tool\n",
        "\n",
        "Now that we've explored the Retrieval Tool - let's try the Code Interpreter tool!\n",
        "\n",
        "The process will be almost exactly the same - but we can explore a different query, and we'll add our file at the Message level!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "IC81y_VtH9lw"
      },
      "outputs": [],
      "source": [
        "ci_assistant = client.beta.assistants.create(\n",
        "  name=name + \"+ Code Interpreter\",\n",
        "  instructions=instructions,\n",
        "  model=model,\n",
        "  tools=[{\"type\": \"code_interpreter\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IPHZswUg6RH"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ali-ce/datasets.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_a8BLVdog1X8"
      },
      "outputs": [],
      "source": [
        "file = client.files.create(\n",
        "  file=open(\"datasets/Y-Combinator/Startups.csv\", \"rb\"),\n",
        "  purpose='assistants'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJPHAJCQJbgi"
      },
      "source": [
        "In the following example, we'll also see how we can package the Thread creation with the Message adding step!\n",
        "\n",
        "> NOTE: Files added at the message/thread level will not be available to the Assistant outside of that Thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "5xVdjH6EJQrr"
      },
      "outputs": [],
      "source": [
        "ci_thread = client.beta.threads.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What kind of file is this?\",\n",
        "      \"attachments\": [\n",
        "          {\n",
        "              \"file_id\" : file.id,\n",
        "              \"tools\" : [{\"type\" : \"code_interpreter\"}]\n",
        "          }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiIYut_dJ0Cv"
      },
      "source": [
        "We'll once again need to create an `EventHandler`, except this time it will have an `on_tool_call_delta` method which will let us see the output of the code interpreter tool as well!\n",
        "\n",
        "> NOTE: Remember that we create runs at the *thread* level - and so don't need the message object to continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "or7iJ492KI2P"
      },
      "outputs": [],
      "source": [
        "class CIEventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_text_delta(self, delta, snapshot):\n",
        "    print(delta.value, end=\"\", flush=True)\n",
        "\n",
        "  def on_tool_call_created(self, tool_call):\n",
        "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
        "\n",
        "  def on_tool_call_delta(self, delta, snapshot):\n",
        "    if delta.type == 'code_interpreter':\n",
        "      if delta.code_interpreter.input:\n",
        "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
        "      if delta.code_interpreter.outputs:\n",
        "        print(f\"\\n\\noutput >\", flush=True)\n",
        "        for output in delta.code_interpreter.outputs:\n",
        "          if output.type == \"logs\":\n",
        "            print(f\"\\n{output.logs}\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPiivot-n5A8"
      },
      "source": [
        "Once again, we can use the streaming interface thanks to creating our `EventHandler`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXsmBqFfiSvT",
        "outputId": "dbfed50c-4e66-4a01-e36a-9651be586be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > code_interpreter\n",
            "\n",
            "import mimetypes\n",
            "\n",
            "# File path\n",
            "file_path = '/mnt/data/file-aUKQ1opZ9ulPMw1tjDGlgXFS'\n",
            "\n",
            "# Determine the file type\n",
            "file_type, _ = mimetypes.guess_type(file_path)\n",
            "file_type\n",
            "assistant > The mimetype of the file is not being identified through `mimetypes.guess_type`. Let's check by examining the file content to get a better idea of the file type.# Try to open the file and read the first few bytes to analyze its content\n",
            "try:\n",
            "    with open(file_path, 'rb') as file:\n",
            "        file_head = file.read(1024)  # Read the first 1024 bytes\n",
            "    \n",
            "    file_head\n",
            "except Exception as e:\n",
            "    str(e)\n",
            "assistant > The file could be binary, or its content might not be directly readable as text. Let's see if we can determine the file type by other methods such as reading the file's metadata or structure. \n",
            "\n",
            "Could you please specify the context or origin of the file, like if it contains text data, images, or some other type of content?"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=ci_thread.id,\n",
        "  assistant_id=ci_assistant.id,\n",
        "  instructions=additional_instructions,\n",
        "  event_handler=CIEventHandler(),\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi10hON2LQmc"
      },
      "source": [
        "And there you go!\n",
        "\n",
        "We've fit our Assistant with an awesome Code Interpreter that lets our Assistant run code on our provided files!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdJxt77oLzu7"
      },
      "source": [
        "### Task 2c: Creating an Assistant with a Function Calling Tool\n",
        "\n",
        "Let's finally create an Assistant that utilizes the Function Calling API.\n",
        "\n",
        "We'll start by creating a function that we wish to be called.\n",
        "\n",
        "We'll utilize DuckDuckGo search to allow our Assistant to have the most up to date information!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5eKEC2wMMVI",
        "outputId": "1712c586-0c40-410c-e2e6-07ea7c918c54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU duckduckgo_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "YPFZ_Uq_LawH"
      },
      "outputs": [],
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def duckduckgo_search(query):\n",
        "  with DDGS() as ddgs:\n",
        "    results = [r for r in ddgs.text(query, max_results=5)]\n",
        "    return \"\\n\".join(result[\"body\"] for result in results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-o1TBFpMSvR"
      },
      "source": [
        "Let's test our function to make sure it behaves as we expect it to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "mCUr9jFCMWBw",
        "outputId": "ea9bb779-f7c3-49e5-b5d4-c12be875a460"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Lowry has been a part of the Jets organization since June 25, 2011, when he was selected in the third round, 67 th overall, after putting up 37 points in 36 games with the Swift Current Broncos of ...\\nLowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ...\\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday. The 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine ...\\nAdam Lowry, who has been a Jet since 2011 when he was drafted 67th overall, is the new captain of the NHL team — its third since relocating to Winnipeg from Atlanta in 2011. Andrew Ladd served ...\\nWinnipeg Jets Captains. Team Names: Winnipeg Jets, Atlanta Thrashers. Seasons: 24 (1999-00 to 2023-24) NHL Playoff Appearances: 8. NHL Championships: 0 (0 Stanley Cup) ... Current Summary/Standings, Current Schedule/Results, Current Leaders, Current Stats. 2023-24, ...'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duckduckgo_search(\"Who is the current captain of the Winnipeg Jets?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzE1nxt5Mi80"
      },
      "source": [
        "Now we need to express how our function works in a way that is compatible with the OpenAI Function Calling API.\n",
        "\n",
        "We'll want to provide a `JSON` object that includes what parameters we have, how to call them, and a short natural language description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "8ElrWvBnMY_s"
      },
      "outputs": [],
      "source": [
        "ddg_function = {\n",
        "    \"name\" : \"duckduckgo_search\",\n",
        "    \"description\" : \"Answer non-technical questions. \",\n",
        "    \"parameters\" : {\n",
        "        \"type\" : \"object\",\n",
        "        \"properties\" : {\n",
        "            \"query\" : {\n",
        "                \"type:\" : \"string\",\n",
        "                \"description\" : \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"\n",
        "            }\n",
        "        },\n",
        "        \"required\" : [\"query\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyRmJgEQVuGs"
      },
      "source": [
        "####❓ Question\n",
        "\n",
        "Why does the description key-value pair matter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tir4WySGM0x2"
      },
      "source": [
        "Now when we create our Assistant - we'll want to include the function description as a tool using the following format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "4eFpwi12Mzlg"
      },
      "outputs": [],
      "source": [
        "fc_assistant = client.beta.assistants.create(\n",
        "    name=name + \" + Function Calling\",\n",
        "    instructions=instructions,\n",
        "    tools=[\n",
        "        {\"type\": \"function\",\n",
        "         \"function\" : ddg_function\n",
        "        }\n",
        "    ],\n",
        "    model=model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcoiF_gXksaa"
      },
      "source": [
        "Now we can create our thread, and attach our message to it - just as we've been doing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "CtELZ5wFjdo_"
      },
      "outputs": [],
      "source": [
        "fc_thread = client.beta.threads.create()\n",
        "fc_message = client.beta.threads.messages.create(\n",
        "  thread_id=fc_thread.id,\n",
        "  role=\"user\",\n",
        "  content=\"Can you describe the Twitter beef between Elon and LeCun?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KT_dj2YkwyK"
      },
      "source": [
        "Once again, we'll need an `EventHandler` for the streaming response - notice that this time we're utilizing a few new methods:\n",
        "\n",
        "1. `handle_requires_action` - this will handle whatever action needs to take place in *our local environment*.\n",
        "2. `submit_tool_outputs` - this will let us submit the resultant outputs from our local function call back to the Assistant run in the required format.\n",
        "\n",
        "To be very clear and explicit - we'll be following this pattern:\n",
        "\n",
        "1. Make a call to the LLM which will decide if a local function call is required.\n",
        "2. If a local function call is required - send a response that indicates a local function call is required.\n",
        "3. Call the function using the arguments provided by the LLM.\n",
        "4. Return the response from the function call with LLM provided arguements.\n",
        "5. Receive a response based on the output of the functional call from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "qgXhMEHbizEX"
      },
      "outputs": [],
      "source": [
        "class FCEventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_event(self, event):\n",
        "    # Retrieve events that are denoted with 'requires_action'\n",
        "    # since these will have our tool_calls\n",
        "    if event.event == 'thread.run.requires_action':\n",
        "      run_id = event.data.id  # Retrieve the run ID from the event data\n",
        "      self.handle_requires_action(event.data, run_id)\n",
        "\n",
        "  def handle_requires_action(self, data, run_id):\n",
        "    tool_outputs = []\n",
        "\n",
        "    for tool in data.required_action.submit_tool_outputs.tool_calls:\n",
        "      print(tool.function.arguments)\n",
        "      if tool.function.name == \"duckduckgo_search\":\n",
        "        tool_outputs.append({\"tool_call_id\": tool.id, \"output\": duckduckgo_search(tool.function.arguments)})\n",
        "\n",
        "    # Submit all tool_outputs at the same time\n",
        "    self.submit_tool_outputs(tool_outputs, run_id)\n",
        "\n",
        "  def submit_tool_outputs(self, tool_outputs, run_id):\n",
        "    # Use the submit_tool_outputs_stream helper\n",
        "    with client.beta.threads.runs.submit_tool_outputs_stream(\n",
        "      thread_id=self.current_run.thread_id,\n",
        "      run_id=self.current_run.id,\n",
        "      tool_outputs=tool_outputs,\n",
        "      event_handler=EventHandler(),\n",
        "    ) as stream:\n",
        "      for text in stream.text_deltas:\n",
        "        print(text, end=\"\", flush=True)\n",
        "      print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Gzz4OZnxCQ"
      },
      "source": [
        "Thanks to our event handler - we can stream this process in our notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HGInY8fjYNa",
        "outputId": "b6fc88c6-4e58-46f2-c21a-0869fc208c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"query\":\"Twitter beef between Elon Musk and LeCun\"}\n",
            "\n",
            "assistant > ItIt looks looks like like I I couldn couldn’t’t fetch fetch the the specific specific details details about about the the Twitter Twitter beef beef between between Elon Elon Musk Musk and and Yann Yann Le LeCCunun.. However However,, I I can can tell tell you you a a bit bit about about these these two two individuals individuals and and what what kind kind of of topics topics they they might might clash clash over over.\n",
            "\n",
            ".\n",
            "\n",
            "ElElonon Musk Musk is is the the CEO CEO of of Tesla Tesla and and Space SpaceXX,, known known for for his his bold bold statements statements and and active active presence presence on on Twitter Twitter.. He He often often speaks speaks on on topics topics related related to to technology technology,, artificial artificial intelligence intelligence ( (AIAI),), and and future future innovations innovations.\n",
            "\n",
            ".\n",
            "\n",
            "YYannann Le LeCCunun is is a a renowned renowned computer computer scientist scientist,, known known for for his his contributions contributions to to deep deep learning learning and and artificial artificial intelligence intelligence.. He He won won the the T Turinguring Award Award and and is is a a prominent prominent figure figure in in the the AI AI community community,, often often sharing sharing his his insights insights and and opinions opinions on on various various technological technological advancements advancements.\n",
            "\n",
            ".\n",
            "\n",
            "TheirTheir clash clash could could be be centered centered around around differences differences in in opinion opinion on on AI AI's's development development and and application application,, risks risks and and benefits benefits of of AI AI,, or or even even on on the the way way AI AI research research should should be be conducted conducted and and governed governed.\n",
            "\n",
            ".\n",
            "\n",
            "IfIf you're you're looking looking for for more more specifics specifics,, you you might might want want to to check check their their recent recent tweets tweets or or news news articles articles covering covering their their interactions interactions on on Twitter Twitter..\n"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=fc_thread.id,\n",
        "  assistant_id=fc_assistant.id,\n",
        "  event_handler=FCEventHandler()\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
